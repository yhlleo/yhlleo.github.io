<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yahui Liu - Researcher in AI</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            line-height: 1.6;
            color: #2c3e50;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            overflow: hidden;
            margin-top: 20px;
            margin-bottom: 20px;
        }

        .header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><defs><pattern id="grain" width="100" height="100" patternUnits="userSpaceOnUse"><circle cx="25" cy="25" r="1" fill="rgba(255,255,255,0.1)"/><circle cx="75" cy="75" r="1" fill="rgba(255,255,255,0.1)"/><circle cx="50" cy="10" r="0.5" fill="rgba(255,255,255,0.05)"/><circle cx="10" cy="60" r="0.5" fill="rgba(255,255,255,0.05)"/></pattern></defs><rect width="100" height="100" fill="url(%23grain)"/></svg>');
            animation: float 20s ease-in-out infinite;
        }

        @keyframes float {
            0%, 100% { transform: translateX(0px) translateY(0px); }
            50% { transform: translateX(-20px) translateY(-20px); }
        }

        .profile-section {
            position: relative;
            z-index: 2;
        }

        .profile-image {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            margin: 0 auto 30px;
            background-image: url('./index_files/YahuiLiu.jpg');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            border: 4px solid rgba(255, 255, 255, 0.3);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .name {
            font-size: 3.5rem;
            font-weight: 700;
            margin-bottom: 10px;
            color: white;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .title {
            font-size: 1.4rem;
            opacity: 0.9;
            margin-bottom: 20px;
            font-weight: 300;
        }

        .contact-info {
            display: flex;
            justify-content: center;
            gap: 30px;
            flex-wrap: wrap;
            margin-top: 30px;
        }

        .contact-item {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 10px 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 25px;
            backdrop-filter: blur(10px);
            transition: all 0.3s ease;
        }

        .contact-item:hover {
            background: rgba(255, 255, 255, 0.2);
            transform: translateY(-2px);
        }

        .main-content {
            padding: 0;
        }

        .nav-tabs {
            display: flex;
            background: #f8f9fa;
            border-bottom: 2px solid #e9ecef;
            overflow-x: auto;
        }

        .nav-tab {
            padding: 20px 30px;
            background: none;
            border: none;
            cursor: pointer;
            font-size: 1.1rem;
            font-weight: 600;
            color: #6c757d;
            transition: all 0.3s ease;
            position: relative;
            white-space: nowrap;
        }

        .nav-tab.active {
            color: #2a5298;
            background: white;
        }

        .nav-tab.active::after {
            content: '';
            position: absolute;
            bottom: -2px;
            left: 0;
            right: 0;
            height: 3px;
            background: linear-gradient(45deg, #2a5298, #667eea);
        }

        .tab-content {
            display: none;
            padding: 40px;
            animation: fadeIn 0.5s ease-in-out;
        }

        .tab-content.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .section-title {
            font-size: 2rem;
            color: #2a5298;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 3px solid #e9ecef;
            position: relative;
        }

        .section-title::after {
            content: '';
            position: absolute;
            bottom: -3px;
            left: 0;
            width: 60px;
            height: 3px;
            background: linear-gradient(45deg, #2a5298, #667eea);
        }

        .about-content p {
            font-size: 1.1rem;
            margin-bottom: 20px;
            text-align: justify;
        }

        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 600;
        }

        .experience-item, .paper-item {
            background: white;
            border-radius: 12px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            border-left: 4px solid #2a5298;
            transition: all 0.3s ease;
        }

        .experience-item:hover, .paper-item:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }

        .experience-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 15px;
            flex-wrap: wrap;
            gap: 10px;
        }

        .experience-title {
            font-size: 1.3rem;
            font-weight: 700;
            color: #2a5298;
        }

        .experience-company {
            font-size: 1.1rem;
            color: #666;
            margin-bottom: 5px;
        }

        .experience-date {
            font-size: 0.95rem;
            color: #888;
            background: #f8f9fa;
            padding: 5px 12px;
            border-radius: 20px;
        }

        .paper-title {
            font-size: 1.2rem;
            font-weight: 700;
            color: #2a5298;
            margin-bottom: 10px;
            line-height: 1.4;
        }

        .paper-authors {
            color: #666;
            margin-bottom: 10px;
            font-size: 1rem;
        }

        .paper-venue {
            font-weight: 600;
            color: #e74c3c;
            margin-bottom: 15px;
        }

        .paper-links {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }

        .paper-link {
            padding: 6px 15px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            text-decoration: none;
            border-radius: 20px;
            font-size: 0.9rem;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }

        .paper-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .news-item {
            padding: 20px;
            background: white;
            border-radius: 10px;
            margin-bottom: 15px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            border-left: 4px solid #28a745;
        }

        .news-date {
            font-weight: 600;
            color: #28a745;
            font-size: 0.95rem;
        }

        .awards-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
        }

        .award-item {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .award-item:hover {
            transform: scale(1.05);
        }

        .award-icon {
            font-size: 2.5rem;
            color: #e67e22;
            margin-bottom: 15px;
        }

        .award-title {
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .services-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 30px;
        }

        .service-category {
            background: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        }

        .category-title {
            font-size: 1.3rem;
            font-weight: 700;
            color: #2a5298;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .service-list {
            list-style: none;
        }

        .service-list li {
            padding: 8px 0;
            border-bottom: 1px solid #f0f0f0;
            font-size: 0.95rem;
        }

        .service-list li:last-child {
            border-bottom: none;
        }

        @media (max-width: 768px) {
            .container {
                margin: 10px;
                border-radius: 15px;
            }

            .header {
                padding: 40px 20px;
            }

            .name {
                font-size: 2.5rem;
            }

            .contact-info {
                flex-direction: column;
                align-items: center;
                gap: 15px;
            }

            .tab-content {
                padding: 20px;
            }

            .experience-header {
                flex-direction: column;
                align-items: flex-start;
            }

            .nav-tabs {
                flex-wrap: wrap;
            }

            .nav-tab {
                padding: 15px 20px;
                font-size: 1rem;
            }
        }

        .footer {
            text-align: center;
            padding: 30px;
            background: #f8f9fa;
            color: #6c757d;
            border-top: 1px solid #e9ecef;
        }
    </style>
    <script>
        function switchTab(event, tabName) {
            // Hide all tab contents
            const tabContents = document.querySelectorAll('.tab-content');
            tabContents.forEach(tab => {
                tab.classList.remove('active');
            });

            // Remove active class from all tabs
            const tabs = document.querySelectorAll('.nav-tab');
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });

            // Show the selected tab content
            document.getElementById(tabName).classList.add('active');

            // Add active class to the clicked tab
            event.target.classList.add('active');
        }
    </script>
</head>
<body>
    <div class="container">
        <header class="header">
            <div class="profile-section">
                <div class="profile-image"></div>
                <h1 class="name">Yahui Liu</h1>
                <p class="title">Researcher in Artificial Intelligence</p>
                <div class="contact-info">
                    <div class="contact-item">
                        <i class="fas fa-building"></i>
                        <span>Kuaishou Technology</span>
                    </div>
                    <div class="contact-item">
                        <i class="fas fa-envelope"></i>
                        <span>yahui.cvrs@gmail.com</span>
                    </div>
                    <div class="contact-item">
                        <i class="fas fa-map-marker-alt"></i>
                        <span>Beijing, China</span>
                    </div>
                    <a href="https://scholar.google.com/citations?user=P8qd0rEAAAAJ&hl=en" target="_blank" class="contact-item" style="text-decoration: none; color: inherit;">
                        <i class="fas fa-graduation-cap"></i>
                        <span>Google Scholar</span>
                    </a>
                    <a href="https://github.com/yhlleo" target="_blank" class="contact-item" style="text-decoration: none; color: inherit;">
                        <i class="fab fa-github"></i>
                        <span>GitHub</span>
                    </a>
                </div>
            </div>
        </header>

        <main class="main-content">
            <nav class="nav-tabs">
                <button class="nav-tab active" onclick="switchTab(event, 'about')">About</button>
                <button class="nav-tab" onclick="switchTab(event, 'experience')">Experience</button>
                <button class="nav-tab" onclick="switchTab(event, 'publications')">Publications</button>
                <button class="nav-tab" onclick="switchTab(event, 'news')">News</button>
                <button class="nav-tab" onclick="switchTab(event, 'services')">Services</button>
                <button class="nav-tab" onclick="switchTab(event, 'awards')">Awards</button>
            </nav>

            <div id="about" class="tab-content active">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <p>
                        I am currently a Researcher at Kuaishou Technology, focusing on cutting-edge research in Computer Vision and Natural Language Processing. My current research interests include Multimodal Large Language Models (MLLMs), Formal Theorem Proving, and AI Agents.
                    </p>
                    <p>
                        I received my Ph.D. degree from the <a href="http://mhug.disi.unitn.it/">Multimedia and Human Understanding Group (MHUG)</a> at the Department of Information Engineering and Computer Science, University of Trento, Italy, in 2022. I was supervised by Prof. <a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en">Nicu Sebe</a> and Dr. <a href="https://scholar.google.com/citations?user=zN5RTZcAAAAJ&hl=en">Bruno Lepri</a>, with my thesis defense committee including <a href="https://scholar.google.com/citations?hl=en&user=yV3_PTkAAAAJ">Vittorio Murino</a>, <a href="https://scholar.google.com/citations?hl=en&user=1I-DKy8AAAAJ">Zhengyou Zhang</a>, and <a href="https://scholar.google.com/citations?hl=en&user=xf1T870AAAAJ">Elisa Ricci</a>.
                    </p>
                    <p>
                        Before my doctoral studies, I earned my B.Eng. degree in Photogrammetry and Remote Sensing (2015) and M.Eng. degree in Pattern Recognition and Intelligent System (2018) from Wuhan University, China.
                    </p>
                    <p>
                        <strong>We are actively recruiting daily interns for long-term positions. Please feel free to submit your resume to my email for exciting research opportunities!</strong>
                    </p>
                </div>
            </div>

            <div id="experience" class="tab-content">
                <h2 class="section-title">Research Experience</h2>
                
                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">Researcher</div>
                            <div class="experience-company">Kuaishou Technology, Beijing, China</div>
                        </div>
                        <div class="experience-date">01/2025 - Present</div>
                    </div>
                    <p>Research focus: MLLMs, Formal Theorem Proving and Agents</p>
                </div>

                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">Researcher</div>
                            <div class="experience-company">Huawei, Shenzhen, China</div>
                        </div>
                        <div class="experience-date">08/2022 - 01/2025</div>
                    </div>
                    <p>Research focus: Image Generation and Enhancing (GANs and Diffusion Models)</p>
                </div>

                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">Research Intern</div>
                            <div class="experience-company">Tencent AI Lab, Shenzhen, China</div>
                        </div>
                        <div class="experience-date">2021 - 06/2022</div>
                    </div>
                    <p>Mentors: Dr. <a href="https://scholar.google.com/citations?user=xQZMbkUAAAAJ&hl=en">Linchao Bao</a> and Dr. <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a>.</p> 
                    <p>Research focus: GANs, Image Domain Translation</p>
                </div>

                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">PhD Student</div>
                            <div class="experience-company">FBK and MHUG, Trento, Italy</div>
                        </div>
                        <div class="experience-date">12/2018 - 06/2022</div>
                    </div>
                    <p>Mentors: Prof. <a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en">Nicu Sebe</a> and Dr. <a href="https://scholar.google.com/citations?user=zN5RTZcAAAAJ&hl=en">Bruno Lepri</a>.</p> 
                    <p>Research focus: Deep learning, GANs, Cross-modal Representations, Image Domain Translation</p>
                </div>

                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">Research Intern</div>
                            <div class="experience-company">Tencent AI Lab, Shenzhen, China</div>
                        </div>
                        <div class="experience-date">11/2017 - 09/2018</div>
                    </div>
                    <p>Mentors: Dr. <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a> and Dr. <a href="https://scholar.google.com/citations?user=ukdqC6IAAAAJ&hl=en">Xiaojiang Liu</a>.</p> 
                    <p>Research focus: Deep Learning, Neural Dialogue Generation</p>
                </div>

                <div class="experience-item">
                    <div class="experience-header">
                        <div>
                            <div class="experience-title">Master Student</div>
                            <div class="experience-company">Computer Vision and Remote Sensing (CVRS) Lab, Wuhan, China</div>
                        </div>
                        <div class="experience-date">03/2015 - 06/2018</div>
                    </div>
                    <p>Mentor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=ZBdFPZYAAAAJ">Jian Yao</a>. </p>
                    <p>Research focus: Deep Learning, Remote Sensing</p>
                </div>
            </div>

            <div id="publications" class="tab-content">
                <h2 class="section-title">Selected Publications</h2>
                
                <h3 style="color: #2a5298; margin-bottom: 20px; font-size: 1.4rem;">Conference Papers</h3>
                
                <div class="paper-item">
                    <div class="paper-title">Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search</div>
                    <div class="paper-authors">Linhao Yu, Xinguang Ji, <strong>Yahui Liu</strong>, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang, V. W., Fuzheng Zhang, Deyi Xiong</div>
                    <div class="paper-venue">Annual Meeting of the Association for Computational Linguistics (ACL), 2025</div>
                    <div class="paper-links">
                        <a href="https://arxiv.org/pdf/2506.11155" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/tjunlp-lab/MCTS-VCB" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-title">Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers</div>
                    <div class="paper-authors">Bin Ren*, <strong>Yahui Liu*</strong>, Yue Song, Wei Bi, Rita Cucchiara, Nicu Sebe and Wei Wang (*equal contribution)</div>
                    <div class="paper-venue">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</div>
                    <div class="paper-links">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Masked_Jigsaw_Puzzle_A_Versatile_Position_Embedding_for_Vision_Transformers_CVPR_2023_paper.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://arxiv.org/abs/2205.12551" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/yhlleo/MJP" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-title">Efficient Training of Visual Transformers with Small Datasets</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, Marco De Nadai</div>
                    <div class="paper-venue">Advances in Neural Information Processing Systems (NeurIPS), 2021</div>
                    <div class="paper-links">
                        <a href="https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e82c-Paper.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://arxiv.org/abs/2106.03746" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://nips.cc/virtual/2021/poster/27009" class="paper-link"><i class="fas fa-image"></i> Poster</a>
                        <a href="https://github.com/yhlleo/VTs-Drloc" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-title">Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang,  Marco De Nadai.</div>
                    <div class="paper-venue">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021</div>
                    <div class="paper-links">
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Smoothing_the_Disentangled_Latent_Style_Space_for_Unsupervised_Image-to-Image_Translation_CVPR_2021_paper.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Liu_Smoothing_the_Disentangled_CVPR_2021_supplemental.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Supplementary</a>
                        <a href="https://drive.google.com/file/d/1PNEym3Zc48trSs6BZzf7s_tkIUC3rGhr/view?usp=sharing" class="paper-link"><i class="fas fa-image"></i> Video </a>
                        <a href="https://github.com/yhlleo/SmoothingLatentSpace" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-title">Describe What to Change: A Text-guided Unsupervised Image-to-Image Translation Approach</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Marco De Nadai, Deng Cai, Huayang Li, Xavier Alameda-Pineda, Nicu Sebe, and Bruno Lepri</div>
                    <div class="paper-venue">ACM International Conference on Multimedia (ACM MM), 2020</div>
                    <div class="paper-links">
                        <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413505" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://arxiv.org/abs/2008.04200" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/yhlleo/DWC-GAN" class="paper-link"><i class="fas fa-image"></i> Code</a>
                    </div>
                </div>


                <h3 style="color: #2a5298; margin: 40px 0 20px; font-size: 1.4rem;">Journal Articles</h3>

                <div class="paper-item">
                    <div class="paper-title">Spatial Entropy as An Inductive Bias for Vision Transformers</div>
                    <div class="paper-authors">Elia Peruzzo, Enver Sangineto, <strong>Yahui Liu</strong>, Marco De Nadai, Wei Bi, Bruno Lepri, Nicu Sebe</div>
                    <div class="paper-venue">Machine Learning, 2024 (Impact Factor: 5.8)</div>
                    <div class="paper-links">
                        <a href="#" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="#" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="#" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>
                
                <div class="paper-item">
                    <div class="paper-title">ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Yajing Chen, Linchao Bao, Nicu Sebe, Bruno Lepri, Marco De Nadai</div>
                    <div class="paper-venue">IEEE Transactions on Multimedia (TMM), 2022 (Impact Factor: 8.4)</div>
                    <div class="paper-links">
                        <a href="#" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="#" class="paper-link"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="#" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>
                
                <div class="paper-item">
                    <div class="paper-title">DeepCrack: A Deep Hierarchical Feature Learning Architecture for Crack Segmentation</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Jian Yao, Rengping Xie, and Li Li</div>
                    <div class="paper-venue">Neurocomputing, 2019 (Impact Factor: 5.5)</div>
                    <div class="paper-links">
                        <a href="http://yhlleo.github.io/papers/DeepCrack-Neurocomputing2019.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/yhlleo/DeepCrack" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-title">RoadNet: Learning to Comprehensively Analyze Road Networks in Complex Urban Scenes From High-Resolution Remotely Sensed Images</div>
                    <div class="paper-authors"><strong>Yahui Liu</strong>, Jian Yao, Xiaohu Lu, Menghan Xia, Xingbo Wang, and Yuan Liu</div>
                    <div class="paper-venue">IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2019 (Impact Factor: 7.5)</div>
                    <div class="paper-links">
                        <a href="http://yhlleo.github.io/papers/RoadNet-TGRS2019.pdf" class="paper-link"><i class="fas fa-file-pdf"></i> Paper</a>
                        <a href="https://github.com/yhlleo/RoadNet" class="paper-link"><i class="fas fa-code"></i> Code</a>
                    </div>
                </div>    
            </div>


            <div id="news" class="tab-content">
                <h2 class="section-title">Recent News</h2>
                
                <div class="news-item">
                    <div class="news-date">July 2025</div>
                    <p>We released <a href="https://arxiv.org/pdf/2507.08649">Leanabell-Prover-V2</a> for verifier-integrated reasoning via RL.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">June 2025</div>
                    <p>We released <a href="https://arxiv.org/abs/2506.13277">SeqPE</a> for universal positional encoding.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">May 2025</div>
                    <p>We released <a href="https://arxiv.org/pdf/2505.22148">LCoT2Tree</a> for uncovering structural patterns in Long CoT.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">May 2025</div>
                    <p>We released <a href="https://www.arxiv.org/pdf/2505.19236">CrEval</a> for evaluating text creativity across diverse domains.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">May 2025</div>
                    <p>We released the <a href="https://friedrichor.github.io/projects/UNITE/">UNITE</a> framework for Multimodal Information Retrieval.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">May 2025</div>
                    <p>One paper accepted to ACL main conference: <a href="https://arxiv.org/pdf/2506.11155">MCTS-VCB</a>.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">April 2025</div>
                    <p>We released <a href="https://arxiv.org/abs/2504.12316">Capybara-VL</a> and <a href="https://arxiv.org/abs/2504.12315">Capybara-Omni</a> at ICLR 2025 SCI-FM workshop - our efficient MLLMs.</p>
                </div>

                <div class="news-item">
                    <div class="news-date">April 2025</div>
                    <p>We released <a href="https://arxiv.org/pdf/2504.06122">Leanabell-Prover</a> achieving the SOTA 59.8% pass@32 on MiniF2F-test.</p>
                </div>
                
            </div>

            <div id="services" class="tab-content">
                <h2 class="section-title">Academic Services</h2>
                
                <div class="services-grid">
                    <div class="service-category">
                        <h3 class="category-title">
                            <i class="fas fa-users"></i>
                            Conference Reviews
                        </h3>
                        <ul class="service-list">
                            <li>ICML 2025</li>
                            <li>AAAI 2025</li>
                            <li>CVPR 2024, 2023, 2022, 2021</li>
                            <li>NeurIPS 2025, 2024, 2023, 2022</li>
                            <li>ACM MM 2025, 2024, 2023, 2022, 2021, 2020</li>
                            <li>ACL/EMNLP 2025, 2024</li>
                            <li>ECCV 2024, 2022</li>
                            <li>ICCV 2023, 2021</li>
                            <li>IJCAI 2022, 2021</li>
                        </ul>
                    </div>

                    <div class="service-category">
                        <h3 class="category-title">
                            <i class="fas fa-journal-whills"></i>
                            Journal Reviews
                        </h3>
                        <ul class="service-list">
                            <li>IEEE TPAMI</li>
                            <li>International Journal of Computer Vision (IJCV)</li>
                            <li>IEEE Transactions on Industrial Informatics (TII)</li>
                            <li>IEEE GRSL</li>
                            <li>IEEE J-STARS</li>
                            <li>IEEE TNNLS</li>
                            <li>Machine Vision and Applications (MVAP)</li>
                            <li>IEEE TMM</li>
                            <li>Pattern Recognition Letters (PRL)</li>
                            <li>Information Fusion</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div id="awards" class="tab-content">
                <h2 class="section-title">Selected Awards</h2>
                
                <div class="awards-grid">
                    <div class="award-item">
                        <div class="award-icon">üèÜ</div>
                        <div class="award-title">Pengcheng Excellent Talents</div>
                        <div class="award-description">Shenzhen, China, 2024</div>
                    </div>

                    <div class="award-item">
                        <div class="award-icon">‚≠ê</div>
                        <div class="award-title">Top Minds (Â§©ÊâçÂ∞ëÂπ¥)</div>
                        <div class="award-description">Huawei, China, 2022</div>
                    </div>

                    <div class="award-item">
                        <div class="award-icon">‚≠ê</div>
                        <div class="award-title">Technical Expert (ÊäÄÊúØÂ§ßÂíñ)</div>
                        <div class="award-description">Tencent AI Lab, China, 2021</div>
                    </div>

                    
