<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <style> .img2{opacity: 0.5;}</style>
    <title>Yahui Liu</title>
    <!-- <link rel="shortcut icon" href="./index_files/favicon.ico"/> -->
    <meta content="Yahui Liu, yhlleo.github.io" name="keywords">
    <link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
    <script async="" src="http://www.google-analytics.com/analytics.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-66888300-1', 'auto');
        ga('send', 'pageview');
    </script>
    <script type="text/javascript" src="./index_files/jquery-1.12.4.min.js"></script>
</head>


<body>
<div id="layout-content" style="margin-top:25px">

<table cellpadding="11px">
    <tbody>
        <tr>
            <td width="720px">
                <div id="toptitle">
                    <h1>Yahui Liu</h1>
                </div>
                <!-- <h4>Researcher</h4> -->
                <pz>
                    Huawei<br>
                    Huawei Industrial Park, Bantian, Longgang District, Shenzhen, China<br>
                    Email: yahui.cvrs [at] gmail.com
                </pz>
                <h3>
                <a href="./index_files/cv.pdf"><icon>Curriculum Vitae</icon></a> &nbsp;&nbsp;
                <a href="https://github.com/yhlleo"><icon>Github</icon></a> &nbsp;&nbsp;
                <!-- <a href="https://www.researchgate.net/profile/Yahui-Liu-7"><icon>ResearchGate</icon></a> &nbsp;&nbsp; -->
                <a href="https://scholar.google.com/citations?user=P8qd0rEAAAAJ&hl=en"><icon>Google Scholar</icon></a>
                </h3>
            </td>
            <td valign="bottom">
                <img src="./index_files/YahuiLiu.jpg" border="0" width="200"><br><br>
            </td>
        </tr>
    </tbody>
</table>


<h2>About Me</h2>
    <p>
    Yahui Liu is now a Principal Engineer at Huawei, Shenzhen, China. 
        He got his certificate of Doctor Degree from the <a href="http://mhug.disi.unitn.it/">Multimedia and Human Understanding Group (MHUG)</a> 
        at the Department of Information Engineering and Computer Science of the University of Trento, Italy, supervised by Prof. <a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en">Nicu Sebe</a> and Dr. <a href="https://scholar.google.com/citations?user=zN5RTZcAAAAJ&hl=en">Bruno Lepri</a>, in 2022. 
        He passed his PhD thesis defense (committee: <a href="https://scholar.google.com/citations?hl=en&user=yV3_PTkAAAAJ">Vittorio Murino</a>, <a href="https://scholar.google.com/citations?hl=en&user=1I-DKy8AAAAJ">Zhengyou Zhang</a>, <a href="https://scholar.google.com/citations?hl=en&user=xf1T870AAAAJ">Elisa Ricci</a>). 
        Before that, he received B.Eng. degree in <i>Photogrammetry and Remote Sensing</i> and M.Eng. degree in <i>Pattern Recognition and Intelligent System</i> from <a href="http://www.whu.edu.cn/">Wuhan University</a> in 2015 and 2018, respectively.  
        His research interests lie in the areas of Computer Vision and Natural Language Processing. 
        <!-- Recently, he has been working on <b>Unsupervised Learning</b>, <b>Image Domain Translation</b>, <b>Vision Transformers</b>, and <b>Diffusion Models</b>. -->
        Recently, he has been working on **MLLMs**, **Formal Reasoning**, **Agents** and **O1/R1**. <br>
    </p>

<h2>Research Experience</h2>
  <ul> 
    <li><b>Huawei</b>, Shenzhen, China. 08/2022 -- 01/2025 <br> 
      Image Generation and Enhancing (GANs and Diffusion Models). </li> 
    <li><b>Tencent AI Lab</b>, Shenzhen, China. 2021 -- 2022 <br> 
      Mentors: Dr. <a href="https://scholar.google.com/citations?user=xQZMbkUAAAAJ&hl=en">Linchao Bao</a> and Dr. <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a>. <br>
      GANs, Image Domain Translation. </li> 
    <li><b>FBK and MHUG</b>, Trento, Italy. 12/2018 -- 06/2022 <br>
      Mentors: Prof. <a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en">Nicu Sebe</a> and Dr. <a href="https://scholar.google.com/citations?user=zN5RTZcAAAAJ&hl=en">Bruno Lepri</a>. <br>
      Deep learning, GANs, Cross-modal Representations, Image Domain Translation. </li>
    <li><b>Tencent AI Lab</b>, Shenzhen, China. 11/2017 -- 09/2018 <br>
      Mentors: Dr. <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ&hl=en">Wei Bi</a> and Dr. <a href="https://scholar.google.com/citations?user=ukdqC6IAAAAJ&hl=en">Xiaojiang Liu</a>.<br> 
      Deep Learning, Neural Dialogue Generation. </li>
    <li>Computer Vision and Remote Sensing (<b>CVRS</b>) Lab, Wuhan, China. 03/2015 -- 06/2018<br> 
      Mentor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=lV0Wxw0AAAAJ">Jian Yao</a>. <br> 
      Deep Learning, Remote Sensing. </li>
  </ul>

<h2>News</h2>
  <ul>
  <li> 4/2025: We released <a href="https://arxiv.org/abs/2504.12316">Capybara-VL</a> and <a href="https://arxiv.org/abs/2504.12315">Capybara-Omni</a>, our efficient multimodal LLMs.
  <li> 4/2025: We released <a href="https://arxiv.org/pdf/2504.06122">Leanabell-Prover</a> achieving the SOTA 59.8% pass@32 on MiniF2F-test.
  <li> 12/2024: One paper was accepted to ICASSP.
  <li> 7/2024: One paper was accepted to Machine Learning.
  <li> 2/2023: One paper was accepted to CVPR.
  <li> 12/2022: One paper was accepted to PRL.
  <li> 8/2022: Started my journey in Huawei. 
  <li> 5/2022: Passed my PhD Thesis defense!
  <li> 3/2022: One paper was accepted to TMM.
  <li> 9/2021: One paper was accepted to NeurIPS.
  <li> 3/2021: One paper was accepted to CVPR. </li>
  <li> 10/2020: One paper was accepted to ICPR. </li>
  <li> 7/2020: Two papers were accepted to ACM MM. </li>
  <li> 7/2019: One paper was accepted to ACM MM.</li>
  <li> 4/2019: One paper was accepted to Sensors. 
  <li> 1/2019: One paper was accepted to Neurocomputing. </li>
  <li> 12/2018: Started my new study journey in the University of Trento, Italy.
  <li> 10/2018: One paper was accepted to TGRS. </li>
  <li> 8/2018: One paper was accepted to EMNLP. </li>
  </ul>

    
<h2>Publications [<a href="https://scholar.google.com/citations?user=P8qd0rEAAAAJ&hl=en"><icon>check full list</icon></a>]</h2>
  
  <h3>&#10022 <i>Conference</i></h3>
  <ul>
    <li type="disc">
    <a href="hhttps://arxiv.org/abs/2404.13579">
    LTOS: Layout-controllable Text-Object Synthesis via Adaptive Cross-attention Fusions</a><br>
    Xiaoran Zhao, Tianhao Wu, Yu Lai, Zhiliang Tian, Zhen Huang, <b>Yahui Liu</b>, Zejiang He, Dongsheng Li.<br>
    <i>To be appeared in IEEE International Conference on Acoustics, Speech and Signal Processing</i> (<strong>ICASSP</strong>), 2025.<br>
    <a href="https://arxiv.org/abs/2404.13579"><button>arXiv</button></a> 
    <!--<button>Code</button>-->
    </li>

    <li type="disc">
    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Masked_Jigsaw_Puzzle_A_Versatile_Position_Embedding_for_Vision_Transformers_CVPR_2023_paper.pdf">
    Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers</a><br>
    Bin Ren*, <b>Yahui Liu</b>*, Yue Song, Wei Bi, Rita Cucchiara, Nicu Sebe and Wei Wang.<br>
      (*denotes equal contribution) <br>
    <i>Appeared in IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (<strong>CVPR</strong>), 2023.<br>
    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Masked_Jigsaw_Puzzle_A_Versatile_Position_Embedding_for_Vision_Transformers_CVPR_2023_paper.pdf"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2205.12551"><button>arXiv</button></a> 
    <a href="https://github.com/yhlleo/MJP"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e82c-Paper.pdf">
    Efficient Training of Visual Transformers with Small Datasets</a><br>
    <b>Yahui Liu</b>, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, Marco De Nadai.<br>
    <i>Advances in Neural Information Processing Systems </i>(<b>NeurIPS</b>), 2021.<br>
    <a href="https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e82c-Paper.pdf"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2106.03746"><button>arXiv</button></a>
    <a href="https://nips.cc/virtual/2021/poster/27009"><button>Poster & Poster</button></a>
    <a href="https://github.com/yhlleo/VTs-Drloc"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Smoothing_the_Disentangled_Latent_Style_Space_for_Unsupervised_Image-to-Image_Translation_CVPR_2021_paper.pdf">
    Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation</a><br>
    <b>Yahui Liu</b>, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang,  Marco De Nadai. <br>
    <i>Appeared in IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (<b>CVPR</b>), 2021. <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Smoothing_the_Disentangled_Latent_Style_Space_for_Unsupervised_Image-to-Image_Translation_CVPR_2021_paper.pdf"><button>Paper</button></a>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Liu_Smoothing_the_Disentangled_CVPR_2021_supplemental.pdf"><button>Supplementary</button></a>
    <a href="https://drive.google.com/file/d/1PNEym3Zc48trSs6BZzf7s_tkIUC3rGhr/view?usp=sharing"><button>Video</button></a>
    <a href="https://github.com/yhlleo/SmoothingLatentSpace"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413505">
    Describe What to Change: A Text-guided Unsupervised Image-to-Image Translation Approach</a><br>
    <b>Yahui Liu</b>, Marco De Nadai, Deng Cai, Huayang Li, Xavier Alameda-Pineda, Nicu Sebe, and Bruno Lepri. <br>
    <i>Appeared in ACM International Conference on Multimedia</i> (<b>ACM MM</b>), 2020. <br>
    <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413505"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2008.04200"><button>arXiv</button></a>
    <a href="https://github.com/yhlleo/DWC-GAN"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://dl.acm.org/doi/10.1145/3394171.3413785">
    Retrieval Guided Unsupervised Multi-domain Image-to-Image Translation</a><br>
    Raul Gomez*, <b>Yahui Liu</b>*, Marco De Nadai, Dimosthenis Karatzas, Nicu Sebe, and Bruno Lepri. <br>
      (*denotes equal contribution) <br>
    <i>Appeared in ACM International Conference on Multimedia</i> (<b>ACM MM</b>), 2020. <br>
    <a href="https://dl.acm.org/doi/10.1145/3394171.3413785"><button>Paper</button></a>
    <a href="http://arxiv.org/abs/2008.04991"><button>arXiv</button></a>
    <a href="https://github.com/yhlleo/RG-UNIT"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://dl.acm.org/doi/10.1145/3343031.3351020">
    Gesture-to-Gesture Translation in the Wild via Category-Independent Conditional Maps</a><br>
    <b>Yahui Liu</b>, Marco De Nadai, Gloria Zen, Nicu Sebe, and Bruno Lepri. <br>
    <i>Appeared in ACM International Conference on Multimedia</i> (<b>ACM MM</b>), 2019. <br>
    <a href="https://dl.acm.org/doi/10.1145/3343031.3351020"><button>Paper</button></a>
    <a href="https://arxiv.org/pdf/1907.05916.pdf"><button>arXiv</button></a>
    <a href="https://github.com/yhlleo/TriangleGAN"><button>Code & dataset</button></a>
    </li>

    <li type="disc">
    <a href="https://www.aclweb.org/anthology/D18-1297.pdf">
    Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method</a><br>
    <b>Yahui Liu</b>, Victoria Bi, Jun Gao, Xiaojiang Liu, Jian Yao, and Shuming Shi. <br>
    <i>Appeared in Conference on Empirical Methods in Natural Language Processing</i> (<b>EMNLP</b>), 2018. <br>
    <a href="https://www.aclweb.org/anthology/D18-1297.pdf"><button>Paper</button></a>
    <a href="https://github.com/yhlleo/Reweighting"><button>Code & dataset</button></a>
    </li>
 
  </ul>    

  <h3>&#10022 <i>Journal</i></h3>
  <ul>
    <li type="disc">
    <a href="https://link.springer.com/article/10.1007/s10994-024-06570-7">
    Spatial Entropy as An Inductive Bias for Vision Transformers</a><br>
    Elia Peruzzo, Enver Sangineto, <b>Yahui Liu</b>, Marco De Nadai, Wei Bi, Bruno Lepri, Nicu Sebe. <br>
    <strong>Machine Learning</strong>, 2024. (Impact factor: 5.8). <br>
    <a href="https://link.springer.com/article/10.1007/s10994-024-06570-7"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2206.04636"><button>arXiv</button></a>
    <a href="https://github.com/helia95/SAR"><button>Code</button></a>
    </li>
      
    <li type="disc">
    <a href="https://www.sciencedirect.com/science/article/pii/S0167865522003920">
    Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer</a><br>
    Yingyi Chen, Xi Shen, <b>Yahui Liu</b>, Qinghua Tao, Johan AK Suykens. <br>
    <i>Pattern Recognition Letters</i>(<strong>PRL</strong>), 2022. (Impact factor: 3.9). <br>
    <a href="https://www.sciencedirect.com/science/article/pii/S0167865522003920"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2207.11971"><button>arXiv</button></a>
    <a href="https://github.com/yingyichen-cyy/JigsawViT"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://ieeexplore.ieee.org/document/9735294">
    ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation</a><br>
    <b>Yahui Liu</b>, Yajing Chen, Linchao Bao, Nicu Sebe, Bruno Lepri, Marco De Nadai. <br>
    <i>IEEE Transactions on Multimedia</i>(<strong>TMM</strong>), 2022. (Impact factor: 8.4). <br>
    <a href="https://ieeexplore.ieee.org/document/9735294"><button>Paper</button></a>
    <a href="https://arxiv.org/abs/2109.12492"><button>arXiv</button></a>
    <a href="https://github.com/yhlleo/stylegan-mmuit"><button>Code</button></a>
    </li>

    <li type="disc">
    <a href="https://ieeexplore.ieee.org/document/9653801?source=authoralert">
    Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images</a><br>
    Lei Ding, Hao Tang, <b>Yahui Liu</b>, Yilei Shi, Lorenzo Bruzzone. <br>
    <i>IEEE Transactions on Image Processing</i>(<strong>TIP</strong>), Volume: 31, Pages: 678-690, 2021. (Impact factor: 10.86) <br>
    <a href="https://ieeexplore.ieee.org/document/9653801?source=authoralert"><button>Paper</button></a>
    <a href="https://arxiv.org/pdf/2102.11262.pdf"><button>arXiv</button></a>
    </li>
    
    <li type="disc">    
    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6427508/">
    Multi-Oriented and Scale-Invariant License Plate Detection Based on Convolutional Neural Networks</a><br>
    Jing Han, Jian Yao, Jiao Zhao, Jingmin Tu, and <b>Yahui Liu</b>. <br>
    <strong>Sensors</strong>, Volume: 19, Issue: 5, Page(s): 1175, 2019. (Impact factor: 3.576). <br>
    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6427508/"><button>Paper</button></a>
    </li>
    
    <li type="disc">
    <a href="http://yhlleo.github.io/papers/DeepCrack-Neurocomputing2019.pdf">
    DeepCrack: A Deep Hierarchical Feature Learning Architecture for Crack Segmentation</a><br>
    <b>Yahui Liu</b>, Jian Yao, Rengping Xie, and Li Li. <br>
    <strong>Neurocomputing</strong>, Volume: 338, Page(s): 139-153, 2019. (Impact factor: 5.5). <br>
    <a href="http://yhlleo.github.io/papers/DeepCrack-Neurocomputing2019.pdf"><button>Paper</button></a>
    <a href="https://github.com/yhlleo/DeepCrack"><button>Code & dataset</button></a>
    </li>

    <li type="disc">
    <a href="http://yhlleo.github.io/papers/RoadNet-TGRS2019.pdf">
    RoadNet: Learning to Comprehensively Analyze Road Networks in Complex Urban Scenes From High-Resolution Remotely Sensed Images</a><br>    
    <b>Yahui Liu</b>, Jian Yao, Xiaohu Lu, Menghan Xia, Xingbo Wang, and Yuan Liu. <br>
    IEEE Transactions on Geoscience and Remote Sensing(<strong>TGRS</strong>), Volume: 57, Issue: 4, Page(s): 2043-2056, 2019. (Impact factor: 7.5). <br>
    <a href="http://yhlleo.github.io/papers/RoadNet-TGRS2019.pdf"><button>Paper</button></a>
    <a href="https://github.com/yhlleo/RoadNet"><button>Code & dataset</button></a>
    </li>

    <li type="disc">
    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580518301237">
    Automatic Multi-image Stitching for Concrete Bridge Inspection by Combining Point and Line Features</a><br>    
    Renping Xie, Jian Yao, Kang Liu, Xiaohu Lu, <b>Yahui Liu</b>, Menghan Xia, and Qifei Zeng. <br>
    <b>Automation in Construction</b>, Volume: 90, Page(s): 265-280, 2018. (Impact factor: 9.6). <br>
    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580518301237"><button>Paper</button></a>
    </li>

    <li type="disc">
    <a href="https://www.mdpi.com/2072-4292/9/7/701">
    Optimal Seamline Detection for Orthoimage Mosaicking by Combining Deep Convolutional Neural Network and Graph Cuts</a><br> 
    Li Li, Jian Yao, <b>Yahui Liu</b>, Wei Yuan, Shuzhu Shi, and Shenggu Yuan. <br>
    <b>Remote Sensing</b>, Volume: 9, Page(s): 701, 2017. (Impact factor: 4.2). <br>
    <a href="https://www.mdpi.com/2072-4292/9/7/701"><button>Paper</button></a>
    </li>
  </ul>
  

<h2>Academic Services</h2>
    <b>Conference Review:</b><br>
    <ul>
        <li type="disc">The Forty-Second International Conference on Machine Learning (<b>ICML 2025</b>) </li>
        <li type="disc"> The Association for the Advancement of Artificial Intelligence (<b>AAAI 2025</b>) </li>
        <li type="disc"> The 31th ACM International Conference on Multimedia (<b>ACM MM 2024</b>) </li>
        <li type="disc"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2024</b>) </li>
        <li type="disc"> Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS 2024</b>) </li>
        <li type="disc"> The 31th ACM International Conference on Multimedia (<b>ACM MM 2023</b>) </li>
        <li type="disc"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2023</b>) </li>
        <li type="disc"> Thirty-seventh Conference on Neural Information Processing Systems (<b>NeurIPS 2023</b>) </li>
        <li type="disc"> Thirty-sixth Conference on Neural Information Processing Systems (<b>NeurIPS 2022</b>) </li>
        <li type="disc"> European Conference on Computer Vision (<b>ECCV 2022</b>) </li>
        <li type="disc"> The 30th ACM International Conference on Multimedia (<b>ACM MM 2022</b>) </li>
        <li type="disc"> The Thirty-ninth International Conference on Machine Learning (<b>ICML 2022</b>) </li>
        <li type="disc"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2022</b>) </li>
        <li type="disc"> The 31th International Joint Conference on Artificial Intelligence (<b>IJCAI 2022</b>) </li>
        <li type="disc"> The 29th ACM International Conference on Multimedia (<b>ACM MM 2021</b>) </li>
        <li type="disc"> IEEE/CVF International Conference on Computer Vision (<b>ICCV 2021</b>) </li>
        <li type="disc"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2021</b>) </li>
        <li type="disc"> The 30th International Joint Conference on Artificial Intelligence (<b>IJCAI 2021</b>) </li>
        <li type="disc"> The 28th ACM International Conference on Multimedia (<b>ACM MM 2020</b>) </li>
    </ul>
    <br>

    <b>Journal Review:</b><br>
    <ul>
        <li type="disc"> IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>) </li>
        <li type="disc"> International Journal of Computer Vision (<b>IJCV</b>) </li>
        <li type="disc"> IEEE Transactions on Industrial Informatics (<b>TII</b>) </li>
        <li type="disc"> IEEE Geoscience and Remote Sensing Letters (<b>GRSL</b>) </li>
        <li type="disc"> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (<b>J-STARS</b>) </li>
        <li type="disc"> IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>) </li>
        <li type="disc"> Machine Vision and Applications (<b>MVAP</b>) </li>
        <li type="disc"> IEEE Transactions on Multimedia (<b>TMM</b>) </li>
        <li type="disc"> Pattern Recognition Letters (<b>PRL</b>) </li>
        <li type="disc"> Information Fusion </li>
    </ul>


<h2>Selected Awards</h2>
    <ul>
    <li type="disc"> Pengcheng Excellent Talents (鹏城优才) from Shenzhen, China, 2024 </li> 
    <li type="disc"> Top Minds (天才少年) offer from Huawei, China, 2022 </li>    
    <li type="disc"> Technical Expert (技术大咖) offer from Tencent AI Lab, China, 2021 </li>
    <li type="disc"> Scholarship of the University of Trento, Italy, 2018 </li>
    <li type="disc"> Excellent Thesis, Hubei Province, China, 2015 (ratio: 3%) </li>
    <li type="disc"> Excellent Undergraduate Students, Wuhan University, China, 2013,2014 </li>    
    <br>
    </ul>

<hr>

<table  cellpadding="11px">
    <tbody>
        <tr>
            <center>
            <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=095df7&w=420&t=m&d=PJI6ZCcOVX0DgjAWBu8f0ZWUfqexOBtUengDuvsQAYM&co=f4f4f4&ct=605c5c"></script>
            </center>
        </tr>
    </tbody>
</table>

<table  cellpadding="11px">
    <tbody>
        <tr>
            <center>
            <img src="./index_files/journey_.png" width="300"><br>
            <font size="3" color="Silver">&copy;Yahui Liu &nbsp;&#8226;&nbsp; Updating Constantly </font>
            </center>
        </tr>
    </tbody>
</table>


</body>
</html>
